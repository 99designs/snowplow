# SnowPlow::Etl

TODO: update this - it's quite out of date...

## Introduction

SnowPlow::Etl is a Ruby gem (built with [Bundler] [bundler]) to run the daily
ETL (extract, transform, load) job which processes the CloudFront access logs
generated by `snowplow.js` and turns them into SnowPlow-format, 
day-partitioned event files, all stored in Amazon S3.

Currently the SnowPlow::Etl gem performs the ETL process on Amazon Elastic 
MapReduce using Hive. A future version of this gem will offer a second ETL 
option using a "vanilla Hadoop" process on EMR. If you want this alternative 
ETL option, please vote for [this issue] [hadoopetl].

## Installation

### Dependencies

To make use of SnowPlow::Etl you will need the following:

1. An [Amazon Web Services] [aws] account
2. SnowPlow tracking successfully implemented on your website. Please see the
   [Tracker Setup Guide] [trackerguide] for help implementing this
3. Git installed. Please see the [Git Installation Guide] [gitguide] as needed  
4. RubyGems installed. Please see the [RubyGems Installation Instructions] [gemsguide]
   as needed
5. Nokogiri installed. Please see the [Installing Nokogiri Guide] [nokogiri] as needed
5. Bundler (a Ruby gem) installed (`gem install bundler`)

### Installation

First checkout the repository (TODO: remove the third line when we merge this 
branch to master):

    $ git clone git://github.com/snowplow/snowplow.git
    $ cd snowplow
    $ git checkout etl-scripts
    
Now install the SnowPlow::ETL gem on your system:

    $ cd hive/snowplow-etl
    $ gem build snowplow-etl.gemspec 
    $ sudo gem install snowplow-etl-0.0.1.gem
    $ bundle install

If you have any problems installing, it may be because of a missing dependency on the Nokogiri library. See the [Installing Nokogiri] [nokogiri] guide for help installing Nokogiri in your system.

Now test that the gem was installed successfully:

    $ bundle exec snowplow-etl --version
    snowplow-etl 0.0.1

Note that the `bundle exec` command will only work when you are inside the 
`snowplow-etl` folder.

### Configuration

SnowPlow::Etl requires a YAML format configuration file to run. There
is a configuration file template available in the repository at 
`hive/snowplow-etl/config/config.yml`. The contents should be as
follows:

    :aws:
      :access_key_id: ADD HERE
      :secret_access_key: ADD HERE
    :buckets:
      :query: ADD HERE
      :serde: ADD HERE
      :in: ADD HERE
      :out: ADD HERE
      :archive: ADD HERE

The `aws` variables should be self-explanatory. The `buckets` variables
are as follows:

* `query` is the bucket to install the ETL job's HiveQL scripts
* `serde` is the bucket to install the ETL job's Hive deserializer
* `in` is the bucket containing the CloudFront access logs to process
* `out` is the bucket to store the SnowPlow-format event files in
* `archive` is the bucket to move the processed CloudFront logs to

All `buckets` variables can include a sub-folder within the bucket as 
required. A trailing slash is optional. For example, the following are
all valid configuration settings:

    :buckets:
      :query: my-snowplow-static/hiveql/
      :serde: my-snowplow-static/jars
      :in: my-snowplow-log-bucket

Please note that all buckets must exist prior to running SnowPlow::Etl,
and currently the `query`, `serde` and `archive` buckets must be in a 
US data center (not in e.g. Europe). This latter point is on account of 
[this bug] [s3bug].

## Usage

### Overview

There are two usage modes for the SnowPlow::Etl gem:

1. **Daily mode** where the gem is run daily to process the last 24 hours
   worth of CloudFront access logs ready for SnowPlow
2. **Catchup mode** where the gem is run across a "datespan" of multiple 
   days to bring processing of the CloudFront access logs up-to-date 

In particular, catchup mode is useful when running SnowPlow::Etl for the 
first time, or when something has gone wrong with daily mode and a day's
processing needs to be rerun.

### Command-line options

Invoke the SnowPlow::Etl gem using Bundler's `bundle exec` syntax:

    $ bundle exec snowplow-etl
    
Note that the `bundle exec` command will only work when you are inside the 
`snowplow-etl` folder.

The command-line options for SnowPlow::Etl look like this:

    Usage: snowplow-etl [options]

    Specific options:
        -c, --config CONFIG              configuration file
        -s, --start YYYY-MM-DD           start date (defaults to yesterday)
        -e, --end YYYY-MM-DD             end date (defaults to yesterday)

    Common options:
        -h, --help                       Show this message
        -v, --version                    Show version
   
Invoking SnowPlow::Etl with just a `--config` option puts it into daily
mode, processing CloudFront log files from **yesterday** only:

    $ bundle exec snowplow-etl --config my-config.yml
 
To run SnowPlow::Etl in catchup mode, you need to specify the start and end
dates as well as the `--config` option, like so:

    $ bundle exec snowplow-etl --config my-config.yml --start 2012-06-20 --end 2012-06-24 

This will run SnowPlow::Etl for the period 20 June 2012 to 24 June 2012
inclusive.

### Scheduling

Once you have the ETL process working smoothly, you can set up a daily cronjob
to automate the daily ETL process. The job should run in the early morning 
(say, 4am) when the full set of CloudFront log files for yesterday have been 
finalised.

The recommended way of running the ETL process as a daily cronjob is by using 
the shell script `bin/snowplow-etl-cron`. You need to edit this script and 
update the two variables:

    BUNDLE_GEMFILE=/path/to/snowplow/hive/snowplow-etl
    ETL_CONFIGFILE=/path/to/your-etl-config.yml

Now, assuming you're using the excellent [cronic] [cronic] as a wrapper for 
your cronjobs, and that both cronic and Bundler are on your path, you can 
configure your cronjob like so:

    0 4   * * *   root    cronic /path/to/snowplow/hive/snowplow-etl/bin/etl-cron

This will run the ETL job daily at 4am, emailing any failures to you via cronic.

## Copyright and license

SnowPlow is copyright 2012 SnowPlow Analytics Ltd. Significant portions of `snowplow.js`
are copyright 2010 Anthon Pang.

Licensed under the [Apache License, Version 2.0] [license] (the "License");
you may not use this software except in compliance with the License.

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

[bundler]: http://gembundler.com/
[hadoopetl]: https://github.com/snowplow/snowplow/issues/17
[aws]: http://aws.amazon.com/ 
[trackerguide]: https://github.com/snowplow/snowplow/wiki/Integrating-SnowPlow-tracking-tags-on-your-website
[gitguide]: http://git-scm.com/book/en/Getting-Started-Installing-Git
[gemsguide]: http://docs.rubygems.org/read/chapter/3
[cronic]: http://habilis.net/cronic/
[s3bug]: https://github.com/snowplow/snowplow/issues/16
[license]: http://www.apache.org/licenses/LICENSE-2.0
[nokogiri]: http://nokogiri.org/tutorials/installing_nokogiri.html
